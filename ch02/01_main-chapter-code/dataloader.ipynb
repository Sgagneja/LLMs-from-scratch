{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6e2a4891-c257-4d6b-afb3-e8fef39d0437",
      "metadata": {
        "id": "6e2a4891-c257-4d6b-afb3-e8fef39d0437"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
        "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f678e62-7bcb-4405-86ae-dce94f494303",
      "metadata": {
        "id": "6f678e62-7bcb-4405-86ae-dce94f494303"
      },
      "source": [
        "# The Main Data Loading Pipeline Summarized"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "070000fc-a7b7-4c56-a2c0-a938d413a790",
      "metadata": {
        "id": "070000fc-a7b7-4c56-a2c0-a938d413a790"
      },
      "source": [
        "The complete chapter code is located in [ch02.ipynb](./ch02.ipynb).\n",
        "\n",
        "This notebook contains the main takeaway, the data loading pipeline without the intermediate steps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b4e8f2d-cb81-41a3-8780-a70b382e18ae",
      "metadata": {
        "id": "2b4e8f2d-cb81-41a3-8780-a70b382e18ae"
      },
      "source": [
        "Packages that are being used in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c7ed6fbe-45ac-40ce-8ea5-4edb212565e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7ed6fbe-45ac-40ce-8ea5-4edb212565e1",
        "outputId": "8ee288ac-5968-4242-bf80-ad4afac827af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.6.0+cu124\n",
            "tiktoken version: 0.9.0\n"
          ]
        }
      ],
      "source": [
        "# NBVAL_SKIP\n",
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0ed4b7db-3b47-4fd3-a4a6-5f4ed5dd166e",
      "metadata": {
        "id": "0ed4b7db-3b47-4fd3-a4a6-5f4ed5dd166e"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size, max_length, stride,\n",
        "                         shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "context_length = 1024\n",
        "\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "\n",
        "batch_size = 8\n",
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=batch_size,\n",
        "    max_length=max_length,\n",
        "    stride=max_length\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "664397bc-6daa-4b88-90aa-e8fc1fbd5846",
      "metadata": {
        "id": "664397bc-6daa-4b88-90aa-e8fc1fbd5846"
      },
      "outputs": [],
      "source": [
        "for batch in dataloader:\n",
        "    x, y = batch\n",
        "\n",
        "    token_embeddings = token_embedding_layer(x)\n",
        "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "\n",
        "    input_embeddings = token_embeddings + pos_embeddings\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d3664332-e6bb-447e-8b96-203aafde8b24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3664332-e6bb-447e-8b96-203aafde8b24",
        "outputId": "aba8b982-6301-48f6-9b78-c65a1735bf48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n",
            "tensor([[[-1.2851,  0.5053,  0.0845,  ..., -1.5528,  1.6002, -0.2618],\n",
            "         [-0.1642, -1.0659,  1.0942,  ..., -0.1391,  0.2748, -1.6291],\n",
            "         [ 0.2614, -0.8781, -0.0823,  ...,  1.1031, -0.8093, -0.4124],\n",
            "         [ 0.3019, -0.1343, -1.9968,  ..., -0.2260, -1.3307, -1.4124]],\n",
            "\n",
            "        [[-2.0559, -0.5012, -0.7567,  ...,  1.1022, -0.2524, -0.8624],\n",
            "         [ 2.2392, -1.6364, -2.7276,  ..., -0.9090,  0.1952, -0.7978],\n",
            "         [-0.0074, -2.0662,  0.3107,  ..., -0.5439,  2.0469, -1.1617],\n",
            "         [ 0.1791, -1.3386,  0.2196,  ..., -2.7103,  0.3729, -1.2332]],\n",
            "\n",
            "        [[-1.9953,  0.2921,  0.8364,  ...,  0.8426, -0.6159,  0.2916],\n",
            "         [-0.0621,  0.4292, -0.5617,  ...,  0.9181, -0.3404, -0.9362],\n",
            "         [ 0.3237,  1.1910, -0.6827,  ..., -1.0069,  2.0160, -0.7648],\n",
            "         [ 1.8451,  1.3011, -1.4160,  ...,  0.2983, -1.8808, -1.3800]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.4172, -1.8281,  0.1171,  ..., -1.0960,  1.7749, -1.2157],\n",
            "         [ 1.2330, -0.4761, -0.5424,  ...,  0.8390, -2.8304,  0.4892],\n",
            "         [ 0.3833, -0.1731,  0.8863,  ..., -1.9113, -0.6267,  0.1667],\n",
            "         [-1.1670,  2.5870,  0.6081,  ..., -0.5401, -2.1669,  0.4927]],\n",
            "\n",
            "        [[-1.9898, -0.7544,  0.8486,  ...,  1.6960, -0.4868, -1.3908],\n",
            "         [ 0.7349, -0.9721,  0.4666,  ...,  0.7509, -0.3439,  0.2675],\n",
            "         [ 0.1072, -0.1505,  0.7694,  ..., -1.6581,  1.1556, -1.9974],\n",
            "         [-0.0498, -0.5065,  0.0659,  ..., -0.3524, -1.9586, -1.7509]],\n",
            "\n",
            "        [[-1.2129,  0.9886, -0.6235,  ...,  2.0713, -0.5433, -0.6340],\n",
            "         [ 1.5913, -1.2726, -1.0423,  ...,  0.9729,  0.5542, -1.2330],\n",
            "         [ 0.0214, -0.7271,  0.8937,  ..., -0.1158,  0.8529, -2.3885],\n",
            "         [ 0.5284, -0.2821, -1.1676,  ...,  1.1994, -0.8081, -1.3983]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(input_embeddings.shape)\n",
        "print(input_embeddings)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}